{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install pyspark\n",
    "#!conda install findspark\n",
    "#!conda install kagglehub\n",
    "#!conda install openjdk=8\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 14:35:00 WARN Utils: Your hostname, Ruikes-MacBook-Pro.local resolves to a loopback address: 127.0.0.1; using 192.168.1.69 instead (on interface en0)\n",
      "24/12/02 14:35:00 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/12/02 14:35:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---+\n",
      "| Name|Age|\n",
      "+-----+---+\n",
      "| John| 30|\n",
      "|Alice| 25|\n",
      "|  Bob| 35|\n",
      "+-----+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"MySparkApp\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Test with a simple DataFrame\n",
    "data = [(\"John\", 30), (\"Alice\", 25), (\"Bob\", 35)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Age\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/rzhang/.cache/kagglehub/datasets/microize/newyork-yellow-taxi-trip-data-2020-2019/versions/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/spark_env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "path = \"/Users/rzhang/.cache/kagglehub/datasets/microize/newyork-yellow-taxi-trip-data-2020-2019/versions/25\"\n",
    "# uncomment the following line to download the file package, size = 10GB\n",
    "#path = kagglehub.dataset_download(\n",
    "#    \"microize/newyork-yellow-taxi-trip-data-2020-2019\"\n",
    "#)\n",
    "print(\"Path to dataset files:\", path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 6, 1, 0, 2, 40), tpep_dropoff_datetime=datetime.datetime(2019, 6, 1, 0, 18, 37), passenger_count=1, trip_distance=2.0, RatecodeID=1, store_and_fwd_flag='N', PULocationID=162, DOLocationID=68, payment_type=1, fare_amount=11.5, extra=3.0, mta_tax=0.5, tip_amount=2.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=17.3, congestion_surcharge=2.5),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 6, 1, 0, 22, 43), tpep_dropoff_datetime=datetime.datetime(2019, 6, 1, 0, 27, 27), passenger_count=1, trip_distance=1.53, RatecodeID=1, store_and_fwd_flag='N', PULocationID=48, DOLocationID=239, payment_type=1, fare_amount=6.5, extra=0.5, mta_tax=0.5, tip_amount=2.0, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=12.3, congestion_surcharge=2.5),\n",
       " Row(VendorID=1, tpep_pickup_datetime=datetime.datetime(2019, 6, 1, 0, 21), tpep_dropoff_datetime=datetime.datetime(2019, 6, 1, 0, 30, 49), passenger_count=1, trip_distance=1.5, RatecodeID=1, store_and_fwd_flag='N', PULocationID=148, DOLocationID=87, payment_type=1, fare_amount=8.5, extra=3.0, mta_tax=0.5, tip_amount=1.5, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=13.8, congestion_surcharge=2.5),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 6, 1, 0, 39, 45), tpep_dropoff_datetime=datetime.datetime(2019, 6, 1, 0, 48, 40), passenger_count=1, trip_distance=1.28, RatecodeID=1, store_and_fwd_flag='N', PULocationID=114, DOLocationID=125, payment_type=1, fare_amount=7.5, extra=0.5, mta_tax=0.5, tip_amount=2.26, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=13.56, congestion_surcharge=2.5),\n",
       " Row(VendorID=2, tpep_pickup_datetime=datetime.datetime(2019, 6, 1, 0, 32, 9), tpep_dropoff_datetime=datetime.datetime(2019, 6, 1, 0, 54, 8), passenger_count=4, trip_distance=3.35, RatecodeID=1, store_and_fwd_flag='N', PULocationID=162, DOLocationID=148, payment_type=1, fare_amount=16.5, extra=0.5, mta_tax=0.5, tip_amount=4.06, tolls_amount=0.0, improvement_surcharge=0.3, total_amount=24.36, congestion_surcharge=2.5)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df = spark.read.csv(path + \"/yellow_tripdata_2019-06.csv\", header=True, inferSchema=True)\n",
    "df = spark.read.csv(\"yellow_tripdata_2019-06.csv\", header=True, inferSchema=True) \\\n",
    "    .sample(withReplacement=False, fraction=0.01, seed=42)\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['VendorID',\n",
       " 'tpep_pickup_datetime',\n",
       " 'tpep_dropoff_datetime',\n",
       " 'passenger_count',\n",
       " 'trip_distance',\n",
       " 'RatecodeID',\n",
       " 'store_and_fwd_flag',\n",
       " 'PULocationID',\n",
       " 'DOLocationID',\n",
       " 'payment_type',\n",
       " 'fare_amount',\n",
       " 'extra',\n",
       " 'mta_tax',\n",
       " 'tip_amount',\n",
       " 'tolls_amount',\n",
       " 'improvement_surcharge',\n",
       " 'total_amount',\n",
       " 'congestion_surcharge']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#print all columns\n",
    "df.columns\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Column Name:\n",
    " \n",
    " 1. 'VendorID'  TPEP Provider ID 1 or 2, not used in this project\n",
    " 2. 'tpep_pickup_datetime'  Pickup date and time\n",
    " 3. 'tpep_dropoff_datetime'  Dropoff date and time\n",
    " 4. 'passenger_count'  Number of passengers\n",
    " 5. 'trip_distance'  Trip distance in miles\n",
    " 6. 'RatecodeID'  Rate code ID: 1=Standard rate, 2=JFK, 3=Newark, 4=Nassau or Westchester, 5=Negotiated fare, 6=Group ride\n",
    " 7. 'store_and_fwd_flag'  Store and forward flag, not used in this project\n",
    " 8. 'PULocationID'  Pickup location ID\n",
    " 9. 'DOLocationID'  Dropoff location ID\n",
    " 10. 'payment_type'  Payment type: 1=Credit card, 2=Cash, 3=No charge, 4=Dispute, 5=Unknown, 6=Voided trip\n",
    " 11. 'fare_amount'  Fare amount\n",
    " 12. 'extra'  Extra charges\n",
    " 13. 'mta_tax'  MTA tax\n",
    " 14. 'tip_amount'  Tip amount\n",
    " 15. 'tolls_amount'  Tolls amount\n",
    " 16. 'improvement_surcharge'  Improvement surcharge\n",
    " 17. 'total_amount'  Total amount\n",
    " 18. 'congestion_surcharge'  Congestion surcharge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: timestamp (nullable = true)\n",
      " |-- tpep_dropoff_datetime: timestamp (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- RatecodeID: integer (nullable = true)\n",
      " |-- store_and_fwd_flag: string (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- payment_type: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- mta_tax: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- tolls_amount: double (nullable = true)\n",
      " |-- improvement_surcharge: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- congestion_surcharge: double (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/12/02 14:35:28 WARN package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows: 69859\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:====================================>                      (5 + 3) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+\n",
      "|summary|          VendorID|   passenger_count|     trip_distance|        RatecodeID|store_and_fwd_flag|      PULocationID|     DOLocationID|       payment_type|       fare_amount|             extra|            mta_tax|        tip_amount|      tolls_amount|improvement_surcharge|      total_amount|congestion_surcharge|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+\n",
      "|  count|             69859|             69859|             69859|             69859|             69859|             69859|            69859|              69859|             69859|             69859|              69859|             69859|             69859|                69859|             69859|               69859|\n",
      "|   mean|1.6467599020884924| 1.570849854707339|3.0562505904750994| 1.059162026367397|              null|161.93920611517484|161.0400377904064|  1.288123219628108|13.565729111496013|1.1530518616069512| 0.4948197082695143|2.2669641706866637|0.4025176426802517|  0.29823931061138786| 19.63545312701248|    2.27550852431326|\n",
      "| stddev|0.5015663590224579|1.2182118766523409|3.9535496274657747|0.8150443771163666|              null| 66.45898972052025|70.38748544872442|0.47936395339328647|12.564842063415828|1.2740575899325712|0.06164086618843608|2.8323215844212504|1.7099367292838785| 0.030742627619111427|15.419833836820162|  0.7288221575781539|\n",
      "|    min|                 1|                 0|               0.0|                 1|                 N|                 1|                1|                  1|            -63.75|              -4.5|               -0.5|              -3.7|             -6.12|                 -0.3|            -65.92|                -2.5|\n",
      "|    max|                 4|                 8|             81.27|                99|                 Y|               265|              265|                  4|             420.0|               7.0|               1.44|             103.5|              49.5|                  0.3|             448.5|                 2.5|\n",
      "+-------+------------------+------------------+------------------+------------------+------------------+------------------+-----------------+-------------------+------------------+------------------+-------------------+------------------+------------------+---------------------+------------------+--------------------+\n",
      "\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|store_and_fwd_flag|PULocationID|DOLocationID|payment_type|fare_amount|extra|mta_tax|tip_amount|tolls_amount|improvement_surcharge|total_amount|congestion_surcharge|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "|       1| 2019-06-01 00:02:40|  2019-06-01 00:18:37|              1|          2.0|         1|                 N|         162|          68|           1|       11.5|  3.0|    0.5|       2.0|         0.0|                  0.3|        17.3|                 2.5|\n",
      "|       2| 2019-06-01 00:22:43|  2019-06-01 00:27:27|              1|         1.53|         1|                 N|          48|         239|           1|        6.5|  0.5|    0.5|       2.0|         0.0|                  0.3|        12.3|                 2.5|\n",
      "|       1| 2019-06-01 00:21:00|  2019-06-01 00:30:49|              1|          1.5|         1|                 N|         148|          87|           1|        8.5|  3.0|    0.5|       1.5|         0.0|                  0.3|        13.8|                 2.5|\n",
      "|       2| 2019-06-01 00:39:45|  2019-06-01 00:48:40|              1|         1.28|         1|                 N|         114|         125|           1|        7.5|  0.5|    0.5|      2.26|         0.0|                  0.3|       13.56|                 2.5|\n",
      "|       2| 2019-06-01 00:32:09|  2019-06-01 00:54:08|              4|         3.35|         1|                 N|         162|         148|           1|       16.5|  0.5|    0.5|      4.06|         0.0|                  0.3|       24.36|                 2.5|\n",
      "+--------+--------------------+---------------------+---------------+-------------+----------+------------------+------------+------------+------------+-----------+-----+-------+----------+------------+---------------------+------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n",
      "Columns: ['VendorID', 'tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'store_and_fwd_flag', 'PULocationID', 'DOLocationID', 'payment_type', 'fare_amount', 'extra', 'mta_tax', 'tip_amount', 'tolls_amount', 'improvement_surcharge', 'total_amount', 'congestion_surcharge']\n",
      "VendorID: IntegerType()\n",
      "tpep_pickup_datetime: TimestampType()\n",
      "tpep_dropoff_datetime: TimestampType()\n",
      "passenger_count: IntegerType()\n",
      "trip_distance: DoubleType()\n",
      "RatecodeID: IntegerType()\n",
      "store_and_fwd_flag: StringType()\n",
      "PULocationID: IntegerType()\n",
      "DOLocationID: IntegerType()\n",
      "payment_type: IntegerType()\n",
      "fare_amount: DoubleType()\n",
      "extra: DoubleType()\n",
      "mta_tax: DoubleType()\n",
      "tip_amount: DoubleType()\n",
      "tolls_amount: DoubleType()\n",
      "improvement_surcharge: DoubleType()\n",
      "total_amount: DoubleType()\n",
      "congestion_surcharge: DoubleType()\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Print the schema (column names and types)\n",
    "df.printSchema()\n",
    "\n",
    "# Get number of rows\n",
    "print(\"Number of rows:\", df.count())\n",
    "\n",
    "# Show basic statistics for numeric columns\n",
    "df.describe().show()\n",
    "\n",
    "# Show the first few rows\n",
    "df.show(5)\n",
    "\n",
    "# Get column names\n",
    "print(\"Columns:\", df.columns)\n",
    "\n",
    "# Get basic data types of columns\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df.schema[col].dataType}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remaining columns: ['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'passenger_count', 'trip_distance', 'RatecodeID', 'PULocationID', 'DOLocationID', 'total_amount']\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|PULocationID|DOLocationID|total_amount|\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+\n",
      "| 2019-06-01 00:02:40|  2019-06-01 00:18:37|              1|          2.0|         1|         162|          68|        17.3|\n",
      "| 2019-06-01 00:22:43|  2019-06-01 00:27:27|              1|         1.53|         1|          48|         239|        12.3|\n",
      "| 2019-06-01 00:21:00|  2019-06-01 00:30:49|              1|          1.5|         1|         148|          87|        13.8|\n",
      "| 2019-06-01 00:39:45|  2019-06-01 00:48:40|              1|         1.28|         1|         114|         125|       13.56|\n",
      "| 2019-06-01 00:32:09|  2019-06-01 00:54:08|              4|         3.35|         1|         162|         148|       24.36|\n",
      "+--------------------+---------------------+---------------+-------------+----------+------------+------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Drop specified columns\n",
    "df_cleaned = df.drop(\n",
    "    'VendorID',\n",
    "    'store_and_fwd_flag',\n",
    "    'fare_amount',\n",
    "    'extra',\n",
    "    'mta_tax',\n",
    "    'tolls_amount',\n",
    "    'improvement_surcharge',\n",
    "    'congestion_surcharge',\n",
    "    'payment_type',\n",
    "    'tip_amount'\n",
    ")\n",
    "\n",
    "# Verify the remaining columns\n",
    "print(\"Remaining columns:\", df_cleaned.columns)\n",
    "\n",
    "# Show a few rows of the cleaned dataset\n",
    "df_cleaned.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------+\n",
      "|LocationID|                Zone|\n",
      "+----------+--------------------+\n",
      "|         1|      Newark Airport|\n",
      "|         2|         Jamaica Bay|\n",
      "|         3|Allerton/Pelham G...|\n",
      "|         4|       Alphabet City|\n",
      "|         5|       Arden Heights|\n",
      "+----------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# for better machine learning result, replace pullocationID and dolocationID with location name\n",
    "\n",
    "df_zone_name = spark.read.csv('taxi+_zone_lookup.csv', header=True, inferSchema=True)\n",
    "df_zone_name = df_zone_name.drop('Borough', 'service_zone')\n",
    "df_zone_name.show(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|RatecodeID|     pickup_location|    dropoff_location|total_amount|\n",
      "+--------------------+---------------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "| 2019-06-01 00:02:40|  2019-06-01 00:18:37|              1|          2.0|         1|        Midtown East|        East Chelsea|        17.3|\n",
      "| 2019-06-01 00:22:43|  2019-06-01 00:27:27|              1|         1.53|         1|        Clinton East|Upper West Side S...|        12.3|\n",
      "| 2019-06-01 00:21:00|  2019-06-01 00:30:49|              1|          1.5|         1|     Lower East Side|Financial Distric...|        13.8|\n",
      "| 2019-06-01 00:39:45|  2019-06-01 00:48:40|              1|         1.28|         1|Greenwich Village...|           Hudson Sq|       13.56|\n",
      "| 2019-06-01 00:32:09|  2019-06-01 00:54:08|              4|         3.35|         1|        Midtown East|     Lower East Side|       24.36|\n",
      "+--------------------+---------------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# 1. First approach: Using RDD operations (closer to traditional MapReduce)\n",
    "# Convert location mapping to dictionary\n",
    "location_dict = dict(df_zone_name.select(\"LocationID\", \"Zone\").rdd.collect())\n",
    "\n",
    "# Create broadcast variable for the mapping\n",
    "location_broadcast = spark.sparkContext.broadcast(location_dict)\n",
    "\n",
    "# Define map functions\n",
    "def map_locations(row):\n",
    "    locations = location_broadcast.value\n",
    "    # Map both pickup and dropoff locations\n",
    "    pu_location = locations.get(row.PULocationID, \"Unknown\")\n",
    "    do_location = locations.get(row.DOLocationID, \"Unknown\")\n",
    "    \n",
    "    return (\n",
    "        row.tpep_pickup_datetime,\n",
    "        row.tpep_dropoff_datetime,\n",
    "        row.passenger_count,\n",
    "        row.trip_distance,\n",
    "        row.RatecodeID,\n",
    "        pu_location,  # Replaced PULocationID\n",
    "        do_location,  # Replaced DOLocationID\n",
    "        row.total_amount\n",
    "    )\n",
    "\n",
    "# Apply the transformation\n",
    "\n",
    "# 1. Convert DataFrame to RDD\n",
    "rdd = df_cleaned.rdd\n",
    "\n",
    "# 2. Apply the mapping function\n",
    "mapped_rdd = rdd.map(map_locations)\n",
    "\n",
    "# 3. Define the schema (column names for the new DataFrame)\n",
    "new_column_names = [\n",
    "    'tpep_pickup_datetime',\n",
    "    'tpep_dropoff_datetime',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'pickup_location',\n",
    "    'dropoff_location',\n",
    "    'total_amount'\n",
    "]\n",
    "\n",
    "# 4. Convert back to DataFrame with the new schema\n",
    "df_mapped = mapped_rdd.toDF(new_column_names)\n",
    "\n",
    "# Show results\n",
    "df_mapped.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 20:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-----------+------------+------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "|pickup_date|pickup_hour|dropoff_date|dropoff_hour|passenger_count|trip_distance|RatecodeID|     pickup_location|    dropoff_location|total_amount|\n",
      "+-----------+-----------+------------+------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "| 2019-06-01|          0|  2019-06-01|           0|              1|          2.0|         1|        Midtown East|        East Chelsea|        17.3|\n",
      "| 2019-06-01|          0|  2019-06-01|           0|              1|         1.53|         1|        Clinton East|Upper West Side S...|        12.3|\n",
      "| 2019-06-01|          0|  2019-06-01|           0|              1|          1.5|         1|     Lower East Side|Financial Distric...|        13.8|\n",
      "| 2019-06-01|          0|  2019-06-01|           0|              1|         1.28|         1|Greenwich Village...|           Hudson Sq|       13.56|\n",
      "| 2019-06-01|          0|  2019-06-01|           0|              4|         3.35|         1|        Midtown East|     Lower East Side|       24.36|\n",
      "+-----------+-----------+------------+------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, to_date\n",
    "\n",
    "# Create new DataFrame with extracted date and hour\n",
    "df_time_processed = df_mapped \\\n",
    "    .withColumn('pickup_date', to_date('tpep_pickup_datetime')) \\\n",
    "    .withColumn('pickup_hour', hour('tpep_pickup_datetime')) \\\n",
    "    .withColumn('dropoff_date', to_date('tpep_dropoff_datetime')) \\\n",
    "    .withColumn('dropoff_hour', hour('tpep_dropoff_datetime')) \\\n",
    "    .drop('tpep_pickup_datetime', 'tpep_dropoff_datetime')\n",
    "\n",
    "# Reorder columns for better readability\n",
    "columns_order = [\n",
    "    'pickup_date',\n",
    "    'pickup_hour',\n",
    "    'dropoff_date',\n",
    "    'dropoff_hour',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'pickup_location',\n",
    "    'dropoff_location',\n",
    "    'total_amount'\n",
    "]\n",
    "\n",
    "df_time_processed = df_time_processed.select(columns_order)\n",
    "\n",
    "# Show the result\n",
    "df_time_processed.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 21:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+------------------+-----------+------------+-------------------+------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "|pickup_date|pickup_day_of_week|pickup_hour|dropoff_date|dropoff_day_of_week|dropoff_hour|passenger_count|trip_distance|RatecodeID|     pickup_location|    dropoff_location|total_amount|\n",
      "+-----------+------------------+-----------+------------+-------------------+------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "| 2019-06-01|          Saturday|          0|  2019-06-01|           Saturday|           0|              1|          2.0|         1|        Midtown East|        East Chelsea|        17.3|\n",
      "| 2019-06-01|          Saturday|          0|  2019-06-01|           Saturday|           0|              1|         1.53|         1|        Clinton East|Upper West Side S...|        12.3|\n",
      "| 2019-06-01|          Saturday|          0|  2019-06-01|           Saturday|           0|              1|          1.5|         1|     Lower East Side|Financial Distric...|        13.8|\n",
      "| 2019-06-01|          Saturday|          0|  2019-06-01|           Saturday|           0|              1|         1.28|         1|Greenwich Village...|           Hudson Sq|       13.56|\n",
      "| 2019-06-01|          Saturday|          0|  2019-06-01|           Saturday|           0|              4|         3.35|         1|        Midtown East|     Lower East Side|       24.36|\n",
      "+-----------+------------------+-----------+------------+-------------------+------------+---------------+-------------+----------+--------------------+--------------------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import date_format, dayofweek\n",
    "\n",
    "# Add day of week (1 = Sunday, 7 = Saturday)\n",
    "df_with_dow = df_time_processed \\\n",
    "    .withColumn('pickup_day_of_week', date_format('pickup_date', 'EEEE')) \\\n",
    "    .withColumn('dropoff_day_of_week', date_format('dropoff_date', 'EEEE'))\n",
    "\n",
    "\n",
    "\n",
    "# Reorder columns\n",
    "columns_order = [\n",
    "    'pickup_date',\n",
    "    'pickup_day_of_week',\n",
    "    'pickup_hour',\n",
    "    'dropoff_date',\n",
    "    'dropoff_day_of_week',\n",
    "    'dropoff_hour',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'RatecodeID',\n",
    "    'pickup_location',\n",
    "    'dropoff_location',\n",
    "    'total_amount'\n",
    "]\n",
    "\n",
    "df_with_dow = df_with_dow.select(columns_order)\n",
    "\n",
    "# Show the result\n",
    "from pyspark.storagelevel import StorageLevel\n",
    "#df_with_dow.persist(StorageLevel.MEMORY_ONLY)\n",
    "df_with_dow.cache()\n",
    "df_with_dow.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#count unique dropoff_hour\n",
    "df_with_dow.select('dropoff_hour').distinct().count()\n",
    "\n",
    "#export to csv\n",
    "df_with_dow.write.csv('df_with_dow.csv', header=True, mode='overwrite')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------------------+---------+\n",
      "|location_pair                                 |frequency|\n",
      "+----------------------------------------------+---------+\n",
      "|{NV, NV}                                      |476      |\n",
      "|{Upper East Side South, Upper East Side North}|430      |\n",
      "|{Upper East Side South, Upper East Side South}|354      |\n",
      "|{Upper East Side North, Upper East Side South}|333      |\n",
      "|{Upper East Side North, Upper East Side North}|332      |\n",
      "|{Upper East Side South, Midtown East}         |197      |\n",
      "|{Upper East Side South, Midtown Center}       |189      |\n",
      "|{Lincoln Square East, Upper West Side South}  |185      |\n",
      "|{Upper West Side South, Upper West Side North}|183      |\n",
      "|{Upper West Side North, Upper West Side South}|181      |\n",
      "+----------------------------------------------+---------+\n",
      "only showing top 10 rows\n",
      "\n",
      "\n",
      "Total unique location pairs: 6544\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import struct, col\n",
    "\n",
    "# 1. Convert DataFrame to RDD and map to (location_pair, 1) format\n",
    "location_pairs_rdd = df_with_dow.rdd.map(\n",
    "    lambda row: (\n",
    "        (row.pickup_location, row.dropoff_location), \n",
    "        1\n",
    "    )\n",
    ")\n",
    "\n",
    "# 2. Reduce by key (location pair) to sum frequencies\n",
    "location_pairs_count = location_pairs_rdd.reduceByKey(lambda x, y: x + y)\n",
    "\n",
    "# 3. Sort by frequency in descending order\n",
    "# FIXED: Changed sortByKey to sortBy and added proper sorting function\n",
    "sorted_pairs = location_pairs_count.sortBy(lambda x: x[1], ascending=False)\n",
    "\n",
    "# 4. Convert back to DataFrame for better display\n",
    "result_df = sorted_pairs.toDF(['location_pair', 'frequency'])\n",
    "#result_df = location_pairs_count.toDF(['location_pair', 'frequency'])\n",
    "\n",
    "# 5. Show top results\n",
    "#print(\"Most common pickup-dropoff location pairs:\")\n",
    "result_df.show(10, truncate=False)\n",
    "\n",
    "# Optional: Get total number of unique pairs\n",
    "print(\"\\nTotal unique location pairs:\", result_df.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "SparkSession.builder.getOrCreate().stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
